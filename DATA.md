## DATASET OVERVIEW  

### BASICS: CONTACT, DISTRIBUTION, ACCESS

>Dataset Name  

Hallucinating References 
- LLM_gen_references: References generated from 6 llms (`GPT-4`, `GPT-3.5-turbo`, `Text-davinci-003`, `Llama-2-7b-chat`, `Llama-2-13b-chat`, `Llama-2-70b-chat`) 
- expert-annotation-study (manual annotations of a random sample of the above dataset sample by the authors of the work) 

>Dataset Description
- The LLM_gen_references dataset consists of a corpus of 1000 article and book references generated by 6 different LMs when queried for book and paper titles on one of 200 computer science concepts defined by the Association for Computing Machinery (ACM), together with a label of either “grounded” or “hallucinated” generated by the Bing search engine API.
- The expert-annotation-study dataset consists of a subset of the LLM_gen_references dataset with human annotations to show the reliability of the Bing labels. Specifically, four expert computer science professionals (the authors of this work) manually labelled 10% of the GPT-4- generated references, demonstrating 99%-100% alignment with the Bing labels. 

>Dataset version number or date  

first version, March 12, 2024 

>Dataset owner/manager contact information

- Ayush Agrawal, t-agrawalay@microsoft.com 
- Lester Mackey, lmackey@microsoft.com 

>How can the dataset be accessed?  
From the open-sourced github repo: https://github.com/microsoft/hallucinating-references

### DATASET CONTENTS  

> What are the contents of this dataset? 

- LLM_gen_references
- The dataset contains the following fields along with their description. 
    - `gen_title`: paper title generated from the LLM 
    - `title`: ACM concept for which the title was generated 
    - `model_answer_main_query`: all the titles (total 5) for the particular ACM concept generated from the LLM. 
    - `bing_return`: Bing search label 
    - `model_ans1-3`: three times sampled author names for the `gen_title` from LLM 
    - `neural_ans1-3_list`: consistency scores as provided by the LLM. The scores measure the consistency of the sampled authors from model_ans1-3 field.
    - `neural_ans1-3_prob`: probability calculated from the scores. The probability scores are calculated using the consistency scores provided by the LLM.  

- Expert Annotation Study:
- The probability scores are calculated using the consistency scores provided by the LLM.  
    - `gen_title`: paper title sampled from the above dataset 
    - `search_url`: google-search url  
    - `label`: label provided by the expert annotator (one of the authors) 

>How many items are in the dataset?  

- References generated: 6000 reference titles (1000 items per model (total 6 models)) 

- Expert Annotation study: 500 human annotations (100 items per Annotator (total 5 annotators))  

>For static datasets: What timeframe does the dataset cover?  

LM generations and Bing labels generated during August 2023 and human annotations collected during September 2023. 

### INTENDED & INAPPROPRIATE USES  

>What are the intended purposes for this dataset?  

To advance research in the detection of hallucinated reference titles by LLMs themselves, without relying on external resources, and to allow other researchers to replicate and build on our results. 

>What are some tasks/purposes that this dataset is not appropriate for?  
This dataset is intended to enable other researchers to reproduce our results or build on them with further research. We do not propose to replace human verification and do not provide any guarantees on 100% hallucination detection.  We also do not provide guarantees that our Bing search labelling is 100% accurate.   

## DETAILS

### DATA COLLECTION PROCEDURES  

>How was the data collected?   

Datasets were collected from the language models (OpenAI and opensource) using their APIs. Labelling was carried out using Bing Search API and verified by human experts.

>Who collected the data?  

Authors of the work collected the entire dataset. 

>Describe considerations taken for responsible and ethical data collection. 

The dataset is collected entirely from the LMs, the Bing API, and the authors. In particular, the annotation study is carried out entirely by the authors. The users should not completely rely on our method for carrying out literature reviews etc. Instead, they should be extremely careful and use LLMs with caution while using model’s output references. 

>Describe procedures for getting explicit consent for data collection and use.

Annotations were performed voluntarily by the authors of this paper. 

### REPRESENTATIVENESS  

>How representative is this dataset? What population(s), contexts is it representative of?   

The LM generations are representative of the following language models collected during August 2023 when queried for book and paper titles on one of 200 computer science concepts defined by the Association for Computing Machinery (ACM): GPT-4, GPT-3.5-turbo, Text-davinci-003, Llama-2-7b-chat, Llama-2-13b-chat and Llama-2-70b-chat. The Bing labels are representative of the Bing API search results during August 2023. The human annotations are representative of the authors, all of whom are computer science researchers. 

>What are known limits to this dataset’s representativeness?  

This dataset only represents computer science references (books and papers) related to 200 ACM concepts. In addition, the human annotations are only representative of the paper’s authors. 

### DATA QUALITY  

>What errors, sources of noise, or redundancies are important for dataset users to be aware of?  

We do not provide guarantees for Bing search to be 100% accurate while labelling the titles. 

>How was the data validated/verified?  

An expert annotation study was conducted to verify the "Grounded/Hallucinated" labels for 100 randomly selected sample points in the dataset. 

>What are potential validity issues a user of this dataset needs to be aware of?

The language model responses were generated by a model, and hence not all generated reference titles are grounded in reality.  In addition, the human annotators did not agree on 100% of the groundedness labels for each generated reference. 

>What are other potential data quality issues a user of this dataset needs to be aware of?  

Some model generations did not follow the instruction template entirely; for example, authors were sometimes wrongly positioned. However, these errors had little impact on the study.  

### PRE-PROCESSING, CLEANING, AND LABELING  

>What pre-processing, cleaning, and/or labeling was done on this dataset?  

Basic preprocessing was carried out where we extracted authors and individual titles from the LM outputs generated. Labelling was done using Bing Search. All this was done by the authors. No crowd workers were involved. 


>Provide a link to the code used to preprocess/clean/label the data, if available.   
 
https://github.com/microsoft/hallucinating-references/blob/main/code/src/generate_references.py


### PRIVACY  

>Is it possible to identify individuals (i.e., one or more natural persons), either directly or indirectly (i.e., in combination with other data) from the dataset?  

One can identify persons using the author names of the book or paper titles if they exist. Papers and author names are publicly available.


### ADDITIONAL DETAILS ON DISTRIBUTION & ACCESS

>For static datasets: What will happen to older versions of the dataset? Will they continue to be maintained?  

They will be retained as part of the previous commits. 

>If this dataset links to data from other sources (e.g., this dataset includes links to content such as social media posts or, news articles, but not the actual content), please specify:  

- What sources  
Google scholar, arXiv etc.

- Whether access to these sources will remain available, and for how long  
Depends on the policies of the above mentioned sources.
